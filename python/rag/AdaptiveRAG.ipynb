{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b604a0",
   "metadata": {},
   "source": [
    "# Adaptive RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beabd2d1",
   "metadata": {},
   "source": [
    "![Adaptive RAG 워크플로우](AdaptiveRAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea6b3a",
   "metadata": {},
   "source": [
    "**Adaptive RAG**는 질문의 특성에 따라 적절한 검색 전략을 동적으로 선택하는 고급 RAG 시스템입니다.\n",
    "\n",
    "### 핵심 구성요소 \n",
    " **Query Classifier (쿼리 분류기)**- 사용자 질문을 분석하여 3가지 카테고리로 분류:  \n",
    "    - `vectorstore`: 내부 문서 검색 필요 (예: LangChain, LangGraph 기술 문서)  \n",
    "    - `web_search`: 웹 검색 필요 (예: 최신 뉴스, 실시간 정보)  \n",
    "    - `direct_answer`: 검색 불필요 (예: 일반 상식, 수학 계산)   \n",
    " **Router (라우터)**- 분류 결과에 따라 적절한 노드로 라우팅- 조건부 엣지를 통한 동적 경로 결정    \n",
    " **Retriever Components (검색 컴포넌트)**   \n",
    "    - **문서 Retriever**: Vector Store에서 관련 문서 검색   \n",
    "    - **Web Search**: 외부 웹 검색 수행  \n",
    "    - **Direct Answer**: 검색 없이 직접 답변 생성    \n",
    "\n",
    "### 워크플로우\n",
    "\n",
    "    ```\n",
    "    사용자 질문    \n",
    "    ↓[Classify Query] ← LLM으로 질문 분류    \n",
    "    ↓[Route Query] ← 조건부 라우팅    \n",
    "    ├─→ [Retrieve 문서s] → [Generate Answer]    \n",
    "    ├─→ [Web Search] → [Generate Answer]    \n",
    "    └─→ [Generate Answer] (직접)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8362ff79",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce25179d",
   "metadata": {},
   "source": [
    "Load and/or check for needed environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import os\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # 중첩 evnet loop 허용\n",
    "from typing import Literal, TypedDict\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import  MermaidDrawMethod\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9855ca7b",
   "metadata": {},
   "source": [
    "## VectorStore & Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "문서 포맷팅\n",
    "\"\"\"\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    if not docs:\n",
    "        return \"문서를 찾을 수 없습니다.\"\n",
    "\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        snippet = doc.page_content\n",
    "        source = doc.metadata.get(\"source\", \"알 수 없음\")\n",
    "        # NOTE: Tip: **문서 간의 구분을 짓는 포맷팅은 굉장히 중요합니다.**\n",
    "        formatted_line = f\"[문서-{i}] (출처: {source}): \" + snippet\n",
    "        formatted.append(formatted_line)\n",
    "    \n",
    "    # NOTE: Tip: **문서 간의 구분을 짓는 포맷팅은 굉장히 중요합니다.**\n",
    "    return \"\\n---\\n\".join(formatted)\n",
    "\n",
    "\n",
    "def format_docs_detailed(docs: List[Document], max_length: int = 1000) -> str:\n",
    "    if not docs:\n",
    "        return \"문서를 찾을 수 없습니다.\"\n",
    "\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        snippet = doc.page_content[:max_length]\n",
    "        metadata = \", \".join([f\"{k}={v}\" for k, v in doc.metadata.items()])\n",
    "        formatted_line = f\"<문서-{i} | 정보: {metadata} | 내용: {snippet}>\"\n",
    "        formatted.append(formatted_line)\n",
    "\n",
    "    return \"\\n---\\n\".join(formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "벡터스토어 설정 (Qdrant 기반)\n",
    "\"\"\"\n",
    "from typing import List, Optional\n",
    "# LangChain OpenAI 임베딩 모델\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# Qdrant 벡터스토어 사용\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "# 임베딩 캐싱을 위한 LangChain-Classic(V1.0 이하) 사용\n",
    "from langchain_classic.embeddings import CacheBackedEmbeddings\n",
    "from langchain_classic.storage import LocalFileStore\n",
    "# 환경변수 로드\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def setup_vectorstore(\n",
    "    documents: List,\n",
    "    embeddings: Optional[OpenAIEmbeddings] = None,\n",
    "    collection_name: str = \"default\",\n",
    ") -> VectorStore:\n",
    "    \"\"\"Qdrant 벡터스토어를 생성하고 채웁니다 (in-memory).\"\"\"\n",
    "    if embeddings is None:\n",
    "        # 기본 임베딩 모델 생성\n",
    "        underlying_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "        # 로컬 파일 기반 캐시 스토어 생성\n",
    "        store = LocalFileStore(\"./cache/embeddings/\")\n",
    "\n",
    "        # CacheBackedEmbeddings로 감싸서 캐싱 활성화\n",
    "        embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "            underlying_embeddings, \n",
    "            store, \n",
    "            namespace=underlying_embeddings.model,\n",
    "        )\n",
    "\n",
    "    # Qdrant client (in-memory)\n",
    "    qdrant_client = QdrantClient(\":memory:\")\n",
    "\n",
    "    # Collection 생성\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "    # VectorStore 생성\n",
    "    vectorstore = QdrantVectorStore(\n",
    "        client=qdrant_client,\n",
    "        collection_name=collection_name,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "\n",
    "    # 문서 추가\n",
    "    vectorstore.add_documents(documents)\n",
    "\n",
    "    print(\n",
    "        f\"✓ VectorStore '{collection_name}' created: {len(documents)} documents indexed\"\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def create_base_retriever(vectorstore: VectorStore, k: int = 5):\n",
    "    \"\"\"기본 similarity retriever를 생성합니다.\"\"\"\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": k},\n",
    "    )\n",
    "    print(f\"✓ Retriever created: top_k={k}\")\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the document from docs/langchain_langgraph_full.md\n",
    "loader = TextLoader(\"docs/langchain_langgraph_full.md\", encoding=\"utf-8\")\n",
    "docs_list = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) # 청킹 전략\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# 벡터 스토어 생성 (Qdrant 사용 - 전체 노트북에서 재사용)\n",
    "vectorstore = setup_vectorstore(documents=doc_splits, collection_name=\"langchain_docs\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "print(f\"✓ 문서 로드 완료: {len(docs_list)}개 원본 문서\")\n",
    "print(f\"✓ 문서 분할 완료: {len(doc_splits)}개 청크\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b146ac1",
   "metadata": {},
   "source": [
    "# Define state schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e9ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adaptive RAG State 및 분류 스키마\n",
    "\"\"\"\n",
    "from typing import Literal, Sequence, Annotated\n",
    "from langchain.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.tools import tool, ToolRuntime  # 새로운 tool, ToolRuntime 방식\n",
    "from langchain.agents import create_agent\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class AdaptiveRAGState(TypedDict):\n",
    "    \"\"\"Adaptive RAG 워크플로우 State 정의.\"\"\"\n",
    "\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]\n",
    "    question: str\n",
    "    query_type: Literal[\"vectorstore\", \"web_search\", \"direct_answer\"]\n",
    "    documents: Sequence[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class QueryClassification(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured output schema for classifying how a user question should be handled.\n",
    "\n",
    "    Purpose\n",
    "        - Normalize the router’s decision into a stable `query_type`.\n",
    "        - Provide a brief, auditable justification that downstream components can log or display.\n",
    "\n",
    "    Fields\n",
    "        - query_type (Literal[\"vectorstore\", \"web_search\", \"direct_answer\"])\n",
    "        Routing decision indicating whether to use internal documents, perform a live web search, or answer directly without retrieval.\n",
    "        Allowed values (lowercase, exact match):\n",
    "            - vectorstore — Use internal/embedded corpora (e.g., product/internal docs, API references, tutorials, design notes).\n",
    "            - web_search — Use public web for time-sensitive or external information (e.g., news, release notes, pricing/policy changes).\n",
    "            - direct_answer — Provide an answer directly without retrieval when the question is self-contained or solvable by reasoning.\n",
    "        - reasoning (str)\n",
    "        A concise justification (1–3 sentences) summarizing the key signals that led to the classification\n",
    "        (e.g., intent, authority needs, recency requirements, specificity). Avoid chain-of-thought; state conclusions only.\n",
    "\n",
    "    Decision Policy (guidelines, not enforced)\n",
    "        - Prefer vectorstore for product/library usage, APIs, internal concepts, and stable specifications.\n",
    "        - Prefer web_search for recent events, releases, pricing/policy/security updates, or content absent from internal corpora.\n",
    "        - Prefer direct_answer for general knowledge, trivial lookups, or logical/derivational tasks that need no external facts.\n",
    "    \"\"\"\n",
    "\n",
    "    query_type: Literal[\"vectorstore\", \"web_search\", \"direct_answer\"] = Field(\n",
    "        description=(\n",
    "            \"Routing decision for how to answer the question. \"\n",
    "            \"'vectorstore': internal/embedded docs (APIs, concepts, tutorials). \"\n",
    "            \"'web_search': public web for timely/external info (news, releases, pricing/policies). \"\n",
    "            \"'direct_answer': answer without retrieval when self-contained or solvable by reasoning.\"\n",
    "        )\n",
    "    )\n",
    "    # 실전에서는 사용하시는걸 비추천합니다. -> 출력이 길어질수록 느려집니다.\n",
    "    # 개발할 때는 유용, 어떤 근거를 가지고 분류를 했는지 일치도 확인이 가능합니다.\n",
    "    reasoning: str = Field(\n",
    "        description=(\n",
    "            \"Short justification (1–3 sentences) explaining the classification. \"\n",
    "            \"Mention key signals such as intent, timeliness/recency needs, specificity, and authority requirements. \"\n",
    "            \"Avoid exposing chain-of-thought; provide summarized rationale only.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "CLASSIFICATION_SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert Query Classifier for a Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "[Task]\n",
    "Analyze a single user message and assign exactly one category — `vectorstore, web_search, or direct_answer` — and provide a concise justification.\n",
    "\n",
    "Use the following category definitions (placeholders will be provided at runtime):\n",
    "\n",
    "1) vectorstore: Questions about {vectorstore_topics}, including synonyms, paraphrases, subtopics, implicit references, and project/product-specific terminology that is likely covered by the vector store.\n",
    "2) web_search: Questions that meet {web_search_criteria}, typically requiring up-to-date, external, or not-in-vectorstore information, or when the user explicitly requests web sources.\n",
    "3) direct_answer: Questions that meet {direct_answer_criteria}, solvable via general reasoning, known facts that do not require retrieval, or text manipulation of content provided directly by the user.\n",
    "\n",
    "[Decision policy (apply in order)]\n",
    "\n",
    "- If the query falls within or is plausibly about {vectorstore_topics}, choose vectorstore (even if the answer seems “easy” to the model).\n",
    "- Else, if the query requires current/external information or explicitly asks to browse/search/cite the web per {web_search_criteria}, choose web_search.\n",
    "- Else, choose direct_answer per {direct_answer_criteria}.\n",
    "\n",
    "[Signals and heuristics]\n",
    "\n",
    "Prefer `vectorstore` when:\n",
    "The query targets proprietary/internal content, project artifacts, product features, code/docs known to be embedded, or asks to “use our docs.”\n",
    "The user expects citations from internal docs or mentions internal identifiers, datasets, repositories, org-specific acronyms, or feature names tied to {vectorstore_topics}.\n",
    "\n",
    "Prefer `web_search` when:\n",
    "The query is time-sensitive (“latest”, “today”, “this year”), mentions news/markets/trends, or asks for statistics subject to change.\n",
    "The topic is outside {vectorstore_topics}, references external entities/sites, or requests verification from multiple sources.\n",
    "The user explicitly requests to “search the web,” “browse,” “open this link,” or “find current sources.”\n",
    "\n",
    "Prefer `direct_answer` when:\n",
    "The task is self-contained (math, logic, coding patterns without external APIs/docs, translation, summarization/rewrite of text provided in the message, grammar/style help).\n",
    "The query asks for general principles or definitions that do not depend on internal or current information and do not require citations.\n",
    "\n",
    "[Edge cases]\n",
    "- Ambiguous/underspecified queries: Classify based on the most likely intent using the above policy; note what is missing in the reason.\n",
    "- Multi-intent queries: Choose the dominant intent. If any major part is in-scope for {vectorstore_topics}, prefer vectorstore; otherwise, if any major part requires external/current info, prefer web_search; else direct_answer.\n",
    "- Safety/Compliance content: Still classify; downstream policies handle refusals or safe-completions.\n",
    "- Language: Classify regardless of user language. Reason should be in concise English.\n",
    "\n",
    "[Constraints]\n",
    "- Do not browse, search, or retrieve content. Only classify.\n",
    "- Be precise and avoid speculation beyond the signals present in the query.\n",
    "- Use short, explicit rationales referencing the key signals (e.g., “mentions internal feature X,” “needs current stats,” “simple arithmetic”).\n",
    "\n",
    "[Output format] (return only this JSON object, no extra text):\n",
    "{{\"category\":\"vectorstore|web_search|direct_answer\",\"reason\":\"<one or two concise sentences be shortend>\"}}\n",
    "\"\"\"\n",
    "\n",
    "VECTORSTORE_TOPICS = (\n",
    "    \"LangChain, LangGraph, Multi-agent 시스템, RAG, 벡터 검색, 기술 문서\"\n",
    ")\n",
    "WEB_SEARCH_CRITERIA = \"최신 뉴스, 실시간 정보 등 웹 검색\"\n",
    "DIRECT_ANSWER_CRITERIA = \"일반 지식, 수학 계산 또는 검색 없이 답변 가능한 사실적 질문\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcdd5de",
   "metadata": {},
   "source": [
    "## Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "도구 정의\n",
    "\n",
    "이 셀에서 정의된 도구들은 노트북 전체에서 재사용됩니다:\n",
    "- search_vectorstore: 벡터 DB 검색\n",
    "- search_web: 웹 검색 (Tavily)\n",
    "- calculate: 계산기\n",
    "- tools: 도구 리스트\n",
    "\"\"\"\n",
    "from langchain_tavily import TavilySearch\n",
    "import ast\n",
    "import operator as op\n",
    "from typing import Any\n",
    "\n",
    "# ========================================\n",
    "# Tool 1: Vectorstore 검색\n",
    "# ========================================\n",
    "\n",
    "@tool\n",
    "def search_vectorstore(query: str) -> str:\n",
    "    \"\"\"\n",
    "    벡터 데이터베이스에서 관련 문서를 검색합니다.\n",
    "\n",
    "    Args:\n",
    "        query: 검색 쿼리\n",
    "\n",
    "    Returns:\n",
    "        검색된 문서 내용\n",
    "    \"\"\"\n",
    "    if \"vectorstore\" not in globals() or vectorstore is None:\n",
    "        return \" vectorstore가 정의되지 않았습니다. Section 1.2의 문서 로드 셀을 먼저 실행하세요.\"\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    docs = retriever.invoke(query)\n",
    "\n",
    "    if not docs:\n",
    "        return \"관련 문서를 찾지 못했습니다.\"\n",
    "\n",
    "    result = \"검색 결과:\\n\\n\"\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        result += f\"문서 {i}:\\n{doc.page_content}\\n\\n\"\n",
    "    return result\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Tool 2: 웹 검색 (Tavily API)\n",
    "# ========================================\n",
    "\n",
    "search_web = TavilySearch()\n",
    "\n",
    "# ========================================\n",
    "# Tool 3: 계산기\n",
    "# ========================================\n",
    "\n",
    "# 안전한 수식 평가를 위한 허용 연산자\n",
    "_ALLOWED_OPERATORS: dict[type[ast.AST], Any] = {\n",
    "    ast.Add: op.add,\n",
    "    ast.Sub: op.sub,\n",
    "    ast.Mult: op.mul,\n",
    "    ast.Div: op.truediv,\n",
    "    ast.Pow: op.pow,\n",
    "    ast.USub: op.neg,\n",
    "}\n",
    "\n",
    "\n",
    "def _eval_expr(expr: ast.Expression) -> float:\n",
    "    \"\"\"안전하게 수식을 평가합니다.\"\"\"\n",
    "    return _eval(expr.body)\n",
    "\n",
    "\n",
    "def _eval(node: ast.AST) -> float:\n",
    "    \"\"\"AST 노드를 재귀적으로 평가합니다.\"\"\"\n",
    "    if isinstance(node, ast.Constant):\n",
    "        return float(node.value)\n",
    "    elif isinstance(node, ast.BinOp):\n",
    "        return _ALLOWED_OPERATORS[type(node.op)](_eval(node.left), _eval(node.right))\n",
    "    elif isinstance(node, ast.UnaryOp):\n",
    "        return _ALLOWED_OPERATORS[type(node.op)](_eval(node.operand))\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported operation: {node}\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    수학 계산을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        expression: 계산할 수식 (예: \"2 + 3 * 4\")\n",
    "\n",
    "    Returns:\n",
    "        계산 결과\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(expression, mode=\"eval\")\n",
    "        result = _eval_expr(tree)\n",
    "        return f\"{expression} = {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"계산 오류: {e}\"\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 도구 리스트\n",
    "# ========================================\n",
    "\n",
    "tools = [search_vectorstore, search_web, calculate]\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7201a9bd",
   "metadata": {},
   "source": [
    "# Define Nodes, Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adaptive RAG 노드 및 라우팅 로직 (Part 1)\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# ========================================\n",
    "# 노드 함수 (Part 1: Classification & Retrieval)\n",
    "# ========================================\n",
    "\n",
    "\n",
    "def retrieve_documents(state: AdaptiveRAGState) -> dict:\n",
    "    \"\"\"벡터 스토어에서 관련 문서를 검색합니다.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    # retriever를 직접 사용하여 Document 객체 리스트를 반환\n",
    "    docs = retriever.invoke(question)\n",
    "\n",
    "    return {\"documents\": docs}\n",
    "\n",
    "\n",
    "def web_search_node(state: AdaptiveRAGState) -> dict:\n",
    "    \"\"\"웹 검색을 통해 최신 정보를 가져옵니다.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    result = search_web.invoke({\"query\": question})\n",
    "\n",
    "    docs = [Document(page_content=result)] if result else []\n",
    "    return {\"documents\": docs}\n",
    "\n",
    "\n",
    "def classify_query(state: AdaptiveRAGState) -> dict:\n",
    "    \"\"\"사용자 질문을 분류하여 후속 라우팅에 사용합니다.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    system_prompt = CLASSIFICATION_SYSTEM_PROMPT.format(\n",
    "        vectorstore_topics=VECTORSTORE_TOPICS,\n",
    "        web_search_criteria=WEB_SEARCH_CRITERIA,\n",
    "        direct_answer_criteria=DIRECT_ANSWER_CRITERIA,\n",
    "    )\n",
    "\n",
    "    classification: QueryClassification = classifier_llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=question),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Adaptive RAG에서는 명시적으로 상태를 업데이트하여 다음 노드의 입력을 제어합니다.\n",
    "    return {\"query_type\": classification.query_type, \"documents\": []}\n",
    "\n",
    "\n",
    "def generate_answer(state: AdaptiveRAGState) -> dict:\n",
    "    \"\"\"검색된 컨텍스트 또는 내부 지식을 기반으로 최종 답변을 생성합니다.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    query_type = state.get(\"query_type\", \"vectorstore\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    if query_type == \"direct_answer\":\n",
    "        direct_response = llm.invoke(\n",
    "            [\n",
    "                SystemMessage(\n",
    "                    content=\"\"\"\n",
    "                [Role]\n",
    "                You are an expert assistant. \n",
    "                Provide concise, well‑supported answers in Korean.\n",
    "\n",
    "                [Objectives]\n",
    "                - Deliver a correct, succinct answer with essential rationale.\n",
    "                - Summarize the answer with brief bullet points when helpful.\n",
    "                - Always end with the exact line: 출처: 내부 지식\n",
    "\n",
    "                [Language]\n",
    "                - Write the final answer in Korean (formal, professional tone).\n",
    "                - Keep technical terms, APIs, and proper nouns in English when clearer; add Korean explanations if needed.\n",
    "\n",
    "                [Answering Policy]\n",
    "                - Be precise and avoid speculation. If information is missing or ambiguous, ask 1–3 focused clarifying questions in Korean before proceeding, or clearly state minimal assumptions.\n",
    "                - Prefer actionable guidance (steps, key commands, or code snippets) when relevant; keep examples minimal and directly tied to the question.\n",
    "                - Do not reveal internal reasoning or chain‑of‑thought; present conclusions and key evidence only.\n",
    "\n",
    "                [Safety and Neutrality]\n",
    "                - Remain neutral, professional, and respectful.\n",
    "                - Flag potential risks, limitations, or prerequisites succinctly when material to correct use.\n",
    "\n",
    "                [Output Format]\n",
    "                - 답변: 2–6 sentences directly addressing the question.\n",
    "                - 요약: 2–5 bullets highlighting the key takeaways.\n",
    "                - 출처: 내부 지식\n",
    "\n",
    "                [Style Guidelines]\n",
    "                - Keep it concise; avoid redundancy.\n",
    "                - Use clear headings and bullets sparingly for readability.\n",
    "                - Use consistent terminology; include versions/dates if relevant to accuracy.\n",
    "\n",
    "                [Examples (format only)]\n",
    "                답변: 해당 기능은 서버 측에서 토큰 단위로 처리되며, 비동기 스트리밍을 통해 클라이언트에 전달됩니다. 설정에서 이벤트 스트림을 활성화하고, 각 청크에 대한 누적 토큰 수를 로깅하는 것이 권장됩니다.\n",
    "                요약:\n",
    "                - 서버 비동기 스트리밍 기반 동작\n",
    "                - 설정에서 이벤트 스트림 활성화 필요\n",
    "                - 청크별 누적 토큰 로깅 권장\n",
    "                출처: 내부 지식\n",
    "                \"\"\"\n",
    "                ),\n",
    "                HumanMessage(content=question),\n",
    "            ]\n",
    "        )\n",
    "        return {\"answer\": direct_response.content}\n",
    "\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "\n",
    "    if not context.strip():\n",
    "        user_prompt = f\"\"\"\n",
    "            [Status]\n",
    "            No external context was provided.\n",
    "\n",
    "            [Request]\n",
    "            Do not guess. Specify the additional information required to answer accurately. Respond in English. Use Markdown.\n",
    "\n",
    "            [Question]\n",
    "            {question}\n",
    "\n",
    "            [Output Format]\n",
    "            - Information Request: 3–8 bullets listing specific missing details\n",
    "            - Minimum Required: 1–2 lines stating the minimal context needed to proceed\n",
    "            - Example (optional): one line showing an example of a helpful input format\n",
    "\n",
    "            [Constraints]\n",
    "            - Return only the sections above; do not attempt to answer the question itself.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        user_prompt = f\"\"\"\n",
    "            [Instruction]\n",
    "            You will receive \"Context Sources\" and a \"Question.\" Follow the System Prompt strictly. Respond in English with inline numeric citations and a final \"Sources\" section. Use Markdown.\n",
    "\n",
    "            [Context Sources]\n",
    "            {context}\n",
    "\n",
    "            [Question]\n",
    "            {question}\n",
    "\n",
    "            [Output Requirements]\n",
    "            - Follow the required format defined in the System Prompt exactly.\n",
    "            - Include inline citations [n] for all factual claims.\n",
    "            - End with a \"Sources\" section mapping citation numbers to metadata.\n",
    "        \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    [Role]\n",
    "    You are a careful, citation-first assistant that answers using only the supplied \"Context Sources.\" Produce accurate, concise answers in English with inline citations.\n",
    "\n",
    "    [Language]\n",
    "    - Write the final answer in Korean (formal, professional tone).\n",
    "    - Keep technical terms or source titles in their original language when appropriate.\n",
    "\n",
    "    [Grounding and Hallucination Control]\n",
    "    - Use only facts present in the Context Sources.\n",
    "    - If the context is insufficient, do not attempt an answer; instead request the additional information needed.\n",
    "    - If sources conflict, briefly note the discrepancy and cite each source involved.\n",
    "\n",
    "    [Authority and Conflict Resolution]\n",
    "    - Prefer primary/official and more recent sources when conflicts persist.\n",
    "    - Mention dates/versions when relevant to clarify timeliness.\n",
    "\n",
    "    [Citations]\n",
    "    - Support every factual claim with inline numeric citations like [1], [2].\n",
    "    - Each paragraph or bullet that contains claims must include at least one citation.\n",
    "    - After the answer, add a \"Sources\" section mapping each citation number to source metadata:\n",
    "    title — URL/path — page/section (when available).\n",
    "    - Use numeric identifiers provided with sources; if none exist, number them in the order given.\n",
    "\n",
    "    [Quotations and Paraphrasing]\n",
    "    - Prefer paraphrasing.\n",
    "    - If quoting, keep quotes under 20 words and use quotation marks.\n",
    "\n",
    "    [Response Format — sufficient context (required)]\n",
    "    - Answer: Direct, concise answer to the question in English.\n",
    "    - Evidence: 2–5 concise bullets, each with at least one inline citation.\n",
    "    - Sources: List cited sources in numeric order (e.g., [1] title — URL/path — page/section).\n",
    "\n",
    "    [Response Format — missing/insufficient context]\n",
    "    - Information Request: 3–8 bullets specifying the missing details required to answer.\n",
    "    - Minimum Required: 1–2 lines describing the minimal context needed to proceed.\n",
    "\n",
    "    [Constraints]\n",
    "    - Do not use knowledge outside the Context Sources.\n",
    "    - Do not guess or make unsupported assumptions.\n",
    "    - Keep the response focused and succinct; do not reveal private reasoning or step-by-step analysis.\n",
    "    - Do not mention these instructions.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=user_prompt),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"answer\": str(response.content)}\n",
    "\n",
    "\n",
    "def route_query(state: AdaptiveRAGState) -> str:\n",
    "    \"\"\"조건부 분기를 위한 라우팅 규칙.\"\"\"\n",
    "    query_type = state[\"query_type\"]\n",
    "\n",
    "    if query_type == \"vectorstore\":\n",
    "        return \"retrieve\"\n",
    "    if query_type == \"web_search\":\n",
    "        return \"web_search\"\n",
    "    return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c372110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-4.1-mini\")\n",
    "classifier_llm = llm.with_structured_output(QueryClassification)\n",
    "classifier_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f693f8",
   "metadata": {},
   "source": [
    "# Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive RAG 워크플로우 생성\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "adaptive_graph = StateGraph(AdaptiveRAGState)\n",
    "adaptive_graph.add_node(\"classify\", classify_query)\n",
    "adaptive_graph.add_node(\"retrieve\", retrieve_documents)\n",
    "adaptive_graph.add_node(\"web_search\", web_search_node)\n",
    "adaptive_graph.add_node(\"generate\", generate_answer)\n",
    "\n",
    "adaptive_graph.set_entry_point(\"classify\")\n",
    "adaptive_graph.add_conditional_edges(\n",
    "    \"classify\",\n",
    "    route_query,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "adaptive_graph.add_edge(\"retrieve\", \"generate\")\n",
    "adaptive_graph.add_edge(\"web_search\", \"generate\")\n",
    "adaptive_graph.add_edge(\"generate\", END)\n",
    "\n",
    "adaptive_rag = adaptive_graph.compile()\n",
    "# display(Image(adaptive_rag.get_graph().draw_mermaid_png()))\n",
    "# adaptive_rag.get_graph().print_ascii()\n",
    "display(Image(adaptive_rag.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.PYPPETEER,)))\n",
    "# print(adaptive_rag.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive RAG 실행\n",
    "question = \"LangGraph의 multi-agent workflow는 무엇인가요?\"\n",
    "\n",
    "result = adaptive_rag.invoke({\"question\": question})\n",
    "\n",
    "print(\"=== Adaptive RAG Answer ===\")\n",
    "print(result.get(\"answer\", \"(응답 없음)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive RAG 실행\n",
    "question = \"LangGraph에서 Tool 사용법을 알려주세요.\"\n",
    "\n",
    "result = adaptive_rag.invoke({\"question\": question})\n",
    "\n",
    "print(\"=== Adaptive RAG Answer ===\")\n",
    "print(result.get(\"answer\", \"(응답 없음)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c2c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive RAG 실행\n",
    "question = \"페이커 근황을 알려주세요..\"\n",
    "\n",
    "result = adaptive_rag.invoke({\"question\": question})\n",
    "\n",
    "print(\"=== Adaptive RAG Answer ===\")\n",
    "print(result.get(\"answer\", \"(응답 없음)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "인용 번호를 하이퍼링크로 변환하는 후처리 함수\n",
    "\"\"\"\n",
    "import re\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def add_citation_links(answer_text: str) -> str:\n",
    "    \"\"\"\n",
    "    답변 텍스트에서 [1], [2] 같은 인용 번호를 클릭 가능한 하이퍼링크로 변환합니다.\n",
    "    \n",
    "    Args:\n",
    "        answer_text: LLM이 생성한 답변 텍스트 (Sources 섹션 포함)\n",
    "    \n",
    "    Returns:\n",
    "        Markdown 형식의 하이퍼링크가 포함된 텍스트\n",
    "    \n",
    "    Example:\n",
    "        Input:  \"LangGraph는 노드로 구성됩니다[1]. 상태를 공유합니다[2].\"\n",
    "        Output: \"LangGraph는 노드로 구성됩니다[[1]](#source-1). 상태를 공유합니다[[2]](#source-2).\"\n",
    "    \"\"\"\n",
    "    # 단계 1: Sources 섹션에 anchor 추가\n",
    "    def add_source_anchors(match):\n",
    "        \"\"\"Sources 섹션의 각 항목에 HTML anchor 추가\"\"\"\n",
    "        num = match.group(1)\n",
    "        rest = match.group(2)\n",
    "        return f'<a id=\"source-{num}\"></a>[{num}]{rest}'\n",
    "    \n",
    "    # Sources 섹션의 [1], [2] 등에 anchor 추가\n",
    "    # 패턴: 줄 시작 + [숫자] + 공백 또는 나머지 텍스트\n",
    "    text_with_anchors = re.sub(\n",
    "        r'^(\\[\\d+\\])(\\s.*)$',\n",
    "        add_source_anchors,\n",
    "        answer_text,\n",
    "        flags=re.MULTILINE\n",
    "    )\n",
    "    \n",
    "    # 단계 2: 본문의 인용 번호를 하이퍼링크로 변환\n",
    "    def convert_citation_to_link(match):\n",
    "        \"\"\"본문의 [1] → [[1]](#source-1) 형식으로 변환\"\"\"\n",
    "        num = match.group(1)\n",
    "        return f'[[{num}]](#source-{num})'\n",
    "    \n",
    "    # Sources 섹션 이전의 본문에서만 변환\n",
    "    # (Sources 섹션 자체는 이미 anchor가 있으므로 제외)\n",
    "    parts = text_with_anchors.split('Sources:', 1)\n",
    "    \n",
    "    if len(parts) == 2:\n",
    "        # Sources 이전 본문만 링크 변환\n",
    "        body = parts[0]\n",
    "        sources_section = parts[1]\n",
    "        \n",
    "        # 본문의 [숫자] 패턴을 하이퍼링크로 변환\n",
    "        body_with_links = re.sub(r'\\[(\\d+)\\]', convert_citation_to_link, body)\n",
    "        \n",
    "        # 재조합\n",
    "        result = body_with_links + 'Sources:' + sources_section\n",
    "    else:\n",
    "        # Sources 섹션이 없으면 전체 변환\n",
    "        result = re.sub(r'\\[(\\d+)\\]', convert_citation_to_link, text_with_anchors)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# 테스트\n",
    "sample_answer = \"\"\"\n",
    "LangGraph는 노드로 구성됩니다[1]. 상태를 공유합니다[2].\n",
    "\n",
    "Sources:\n",
    "[1] LangGraph documentation — /oss/langgraph\n",
    "[2] Tutorial — /oss/tutorial\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== 원본 ===\")\n",
    "print(sample_answer)\n",
    "\n",
    "print(\"\\n=== 변환 결과 ===\")\n",
    "converted = add_citation_links(sample_answer)\n",
    "print(converted)\n",
    "\n",
    "print(\"\\n=== Markdown 렌더링 (Jupyter에서만 작동) ===\")\n",
    "display(Markdown(converted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s0m6aqibn4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 Adaptive RAG 답변에 적용\n",
    "answer_with_links = add_citation_links(result.get(\"answer\", \"\"))\n",
    "\n",
    "print(\"=== 하이퍼링크가 추가된 답변 (Markdown) ===\\n\")\n",
    "display(Markdown(answer_with_links))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
