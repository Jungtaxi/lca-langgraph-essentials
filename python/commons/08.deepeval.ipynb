{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef51d44",
   "metadata": {},
   "source": [
    "# DeepEval: LLMÏùÑ ÏúÑÌïú Ïú†Îãõ ÌÖåÏä§Ìä∏ ÎèÑÍµ¨\n",
    "- https://discuss.pytorch.kr/t/deepeval-llm/2557\n",
    "- https://www.reddit.com/r/Python/comments/1i2kafp/deepeval_the_opensource_llm_evaluation_framework/\n",
    "- https://deepeval.com/integrations/frameworks/langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e2076dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6be5a2f",
   "metadata": {},
   "source": [
    "## End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61e82dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f20fb592c1471f9779c7b2ca84d263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the weather in Paris, France?\n",
      "  - actual output: {'messages': [HumanMessage(content='What is the weather in Paris, France?', additional_kwargs={}, response_metadata={}, id='c74ed893-1bd8-428c-8d24-9938634d7721'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 59, 'total_tokens': 73, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_9766e549b2', 'id': 'chatcmpl-CjQSFuZwi32sFyzlIPDMO83MlTDTl', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019aeec5-16a9-70f3-a239-7a96cdb64692-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'Paris'}, 'id': 'call_N43w7eU2uHIAiUez54V7RXo7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 59, 'output_tokens': 14, 'total_tokens': 73, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"It's always sunny in Paris!\", name='get_weather', id='d49ea934-12e3-43a2-b622-b56cd1f1dbae', tool_call_id='call_N43w7eU2uHIAiUez54V7RXo7'), AIMessage(content='The weather in Paris, France is sunny. Would you like to know the weather for another city as well?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 87, 'total_tokens': 110, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_9766e549b2', 'id': 'chatcmpl-CjQSGPYsslEwvA34V0yGp6RzBcuV9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019aeec5-1c41-7c71-b318-5c6ccc03df52-0', usage_metadata={'input_tokens': 87, 'output_tokens': 23, 'total_tokens': 110, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Task Completion (score: 0.5, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The system provided a weather response for Paris, but the answer was generic ('It's always sunny in Paris!') and not an actual current weather report. The user requested the current weather, which requires up-to-date and accurate information, not a placeholder or joke response. The system did follow the process of invoking a weather tool, but the output did not fulfill the user's request for real data., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: {'messages': [{'role': 'user', 'content': 'What is the weather in Paris, France?'}]}\n",
      "  - actual output: {'messages': [HumanMessage(content='What is the weather in Paris, France?', additional_kwargs={}, response_metadata={}, id='c74ed893-1bd8-428c-8d24-9938634d7721'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 59, 'total_tokens': 73, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_9766e549b2', 'id': 'chatcmpl-CjQSFuZwi32sFyzlIPDMO83MlTDTl', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019aeec5-16a9-70f3-a239-7a96cdb64692-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'Paris'}, 'id': 'call_N43w7eU2uHIAiUez54V7RXo7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 59, 'output_tokens': 14, 'total_tokens': 73, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"It's always sunny in Paris!\", name='get_weather', id='d49ea934-12e3-43a2-b622-b56cd1f1dbae', tool_call_id='call_N43w7eU2uHIAiUez54V7RXo7'), AIMessage(content='The weather in Paris, France is sunny. Would you like to know the weather for another city as well?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 87, 'total_tokens': 110, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_9766e549b2', 'id': 'chatcmpl-CjQSGPYsslEwvA34V0yGp6RzBcuV9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019aeec5-1c41-7c71-b318-5c6ccc03df52-0', usage_metadata={'input_tokens': 87, 'output_tokens': 23, 'total_tokens': 110, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the weather in Bogot√°, Colombia?\n",
      "  - actual output: {'messages': [HumanMessage(content='What is the weather in Bogot√°, Colombia?', additional_kwargs={}, response_metadata={}, id='6132b75d-707b-411b-9a12-bc85f2871f98'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 59, 'total_tokens': 74, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_9766e549b2', 'id': 'chatcmpl-CjQSDFFODvrN64I8XqZbr7p12jP7a', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019aeec5-0cfc-7051-9b41-431b2a489318-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'Bogot√°'}, 'id': 'call_JmvvQJlx0T5jb3I3NrIBgisK', 'type': 'tool_call'}], usage_metadata={'input_tokens': 59, 'output_tokens': 15, 'total_tokens': 74, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"It's always sunny in Bogot√°!\", name='get_weather', id='19bcd261-cf7a-4e79-803f-0284fe6b37c8', tool_call_id='call_JmvvQJlx0T5jb3I3NrIBgisK'), AIMessage(content='The weather in Bogot√°, Colombia is always sunny! If you need more detailed information or a forecast, please let me know!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 88, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_9766e549b2', 'id': 'chatcmpl-CjQSEpFluEkIVuSd8hOtUeywlVfLY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019aeec5-10ec-7ec1-96eb-e81569fdf8ef-0', usage_metadata={'input_tokens': 88, 'output_tokens': 26, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Task Completion (score: 0.3, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The system responded to the user's request by providing a weather description for Bogot√°, but the information given ('It's always sunny in Bogot√°!') is inaccurate and generic, not reflecting actual or current weather conditions. The response also did not provide real-time or detailed weather data as typically expected for such a request. However, the system did recognize the user's intent and attempted to answer, which is why the score is not zero., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: {'messages': [{'role': 'user', 'content': 'What is the weather in Bogot√°, Colombia?'}]}\n",
      "  - actual output: {'messages': [HumanMessage(content='What is the weather in Bogot√°, Colombia?', additional_kwargs={}, response_metadata={}, id='6132b75d-707b-411b-9a12-bc85f2871f98'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 59, 'total_tokens': 74, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_9766e549b2', 'id': 'chatcmpl-CjQSDFFODvrN64I8XqZbr7p12jP7a', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019aeec5-0cfc-7051-9b41-431b2a489318-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'Bogot√°'}, 'id': 'call_JmvvQJlx0T5jb3I3NrIBgisK', 'type': 'tool_call'}], usage_metadata={'input_tokens': 59, 'output_tokens': 15, 'total_tokens': 74, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"It's always sunny in Bogot√°!\", name='get_weather', id='19bcd261-cf7a-4e79-803f-0284fe6b37c8', tool_call_id='call_JmvvQJlx0T5jb3I3NrIBgisK'), AIMessage(content='The weather in Bogot√°, Colombia is always sunny! If you need more detailed information or a forecast, please let me know!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 88, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_9766e549b2', 'id': 'chatcmpl-CjQSEpFluEkIVuSd8hOtUeywlVfLY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019aeec5-10ec-7ec1-96eb-e81569fdf8ef-0', usage_metadata={'input_tokens': 88, 'output_tokens': 26, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Task Completion: 50.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">‚ö† WARNING:</span> No hyperparameters logged.\n",
       "¬ª <a href=\"https://deepeval.com/docs/evaluation-prompts\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Log hyperparameters</span></a> to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33m‚ö† WARNING:\u001b[0m No hyperparameters logged.\n",
       "¬ª \u001b]8;id=533338;https://deepeval.com/docs/evaluation-prompts\u001b\\\u001b[1;34mLog hyperparameters\u001b[0m\u001b]8;;\u001b\\ to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Evaluation completed üéâ! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.</span>87s | token cost: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span> USD<span style=\"font-weight: bold\">)</span>\n",
       "¬ª Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
       "   ¬ª Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">1</span>\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Evaluation completed üéâ! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m12.\u001b[0m87s | token cost: \u001b[3;35mNone\u001b[0m USD\u001b[1m)\u001b[0m\n",
       "¬ª Test Results \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
       "   ¬ª Pass Rate: \u001b[1;36m50.0\u001b[0m% | Passed: \u001b[1;32m1\u001b[0m | Failed: \u001b[1;31m1\u001b[0m\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from deepeval.integrations.langchain import CallbackHandler\n",
    "from deepeval.metrics import TaskCompletionMetric\n",
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Returns the weather in a city\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\",  \n",
    "    tools=[get_weather],  \n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "dataset = EvaluationDataset(goldens=[\n",
    "    Golden(input=\"What is the weather in Bogot√°, Colombia?\"),\n",
    "    Golden(input=\"What is the weather in Paris, France?\"),\n",
    "])\n",
    " \n",
    "for golden in dataset.evals_iterator():\n",
    "    agent.invoke(\n",
    "        input={\"messages\": [{\"role\": \"user\", \"content\": golden.input}]},\n",
    "        config={\"callbacks\": [CallbackHandler(metrics=[TaskCompletionMetric()])]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b6013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56e7cbf8",
   "metadata": {},
   "source": [
    "## Component Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476190f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# from deepeval.metrics import AnswerRelevancyMetric\n",
    "# from rich import print as rprint\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-4o-mini\", \n",
    "#     metadata={\"metric\": [AnswerRelevancyMetric()]}\n",
    "# ).bind_tools([get_weather])\n",
    "\n",
    "# agent = create_agent(\n",
    "#     model=\"openai:gpt-4.1-mini\",  \n",
    "#     tools=[get_weather],  \n",
    "#     system_prompt=\"You are a helpful assistant\",\n",
    "# )\n",
    "\n",
    "# dataset = EvaluationDataset(goldens=[\n",
    "#     Golden(input=\"What is the weather in Bogot√°, Colombia?\"),\n",
    "#     Golden(input=\"What is the weather in Paris, France?\"),\n",
    "# ])\n",
    " \n",
    "# for golden in dataset.evals_iterator:\n",
    "#     result = agent.invoke(\n",
    "#         input={\"messages\": [{\"role\": \"user\", \"content\": golden.input}]},\n",
    "#         config={\"callbacks\": [CallbackHandler(metrics=[TaskCompletionMetric()])]}\n",
    "#     )\n",
    "#     rprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01ffe810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87354ab6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tool() got an unexpected keyword argument 'metric'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepeval\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlangchain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tool\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepeval\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AnswerRelevancyMetric\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;129;43m@tool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mAnswerRelevancyMetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mget_weather\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43;03m\"\"\"Get the current weather in a location.\"\"\"\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms always sunny in \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlocation\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/side-job/multi-agent-lecture/ready-for-lect/lca-langgraph-essentials/python/.venv/lib/python3.13/site-packages/deepeval/integrations/langchain/patch.py:21\u001b[39m, in \u001b[36mtool.<locals>.decorator\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorator\u001b[39m(func: Callable) -> BaseTool:\n\u001b[32m     20\u001b[39m     func = _patch_tool_decorator(func, metrics, metric_collection)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     tool_instance = \u001b[43moriginal_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m(func)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tool_instance\n",
      "\u001b[31mTypeError\u001b[39m: tool() got an unexpected keyword argument 'metric'"
     ]
    }
   ],
   "source": [
    "# from langchain_core.tools import tool\n",
    "from deepeval.integrations.langchain import tool\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "\n",
    "@tool(metric=[AnswerRelevancyMetric()])\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the current weather in a location.\"\"\"\n",
    "    return f\"It's always sunny in {location}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\",  \n",
    "    tools=[get_weather],  \n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "dataset = EvaluationDataset(goldens=[\n",
    "    Golden(input=\"What is the weather in Bogot√°, Colombia?\"),\n",
    "    Golden(input=\"What is the weather in Paris, France?\"),\n",
    "])\n",
    " \n",
    "for golden in dataset.evals_iterator():\n",
    "    agent.invoke(\n",
    "        input={\"messages\": [{\"role\": \"user\", \"content\": golden.input}]},\n",
    "        config={\"callbacks\": [CallbackHandler(metrics=[TaskCompletionMetric()])]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e820f33",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Faithfulness\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Setup LLM\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m client = \u001b[43mAsyncOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m llm = llm_factory(\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m, client=client)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create metric\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/side-job/multi-agent-lecture/ready-for-lect/lca-langgraph-essentials/python/.venv/lib/python3.13/site-packages/openai/_client.py:488\u001b[39m, in \u001b[36mAsyncOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    486\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    489\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    490\u001b[39m     )\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_key = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import AsyncOpenAI\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.metrics.collections import Faithfulness\n",
    "\n",
    "# Setup LLM\n",
    "client = AsyncOpenAI()\n",
    "llm = llm_factory(\"gpt-4o-mini\", client=client)\n",
    "\n",
    "# Create metric\n",
    "scorer = Faithfulness(llm=llm)\n",
    "\n",
    "# Evaluate\n",
    "result = await scorer.ascore(\n",
    "    user_input=\"When was the first super bowl?\",\n",
    "    response=\"The first superbowl was held on Jan 15, 1967\",\n",
    "    retrieved_contexts=[\n",
    "        \"The First AFL‚ÄìNFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"\n",
    "    ]\n",
    ")\n",
    "print(f\"Faithfulness Score: {result.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53de95f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
